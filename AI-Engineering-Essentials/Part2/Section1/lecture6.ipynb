{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b0c535",
   "metadata": {},
   "source": [
    "# Lecture 6: Loading Models with `from_pretrained`\n",
    "In this notebook, we will explore how to load pre-trained models using the `from_pretrained` method from the Hugging Face Transformers library. We will also dive into the configuration, weights, and caching mechanisms.\n",
    "\n",
    "The Google Colab versin of this notebook is available here: https://colab.research.google.com/drive/1gb3hu83Wktk5cUDObPJqjlM5olAhbvam?usp=sharing\n",
    "\n",
    "\n",
    "## **Feeling Brave?**\n",
    "\n",
    "Try out the code for this lecture on your laptop or desktop. The same code in the above mentioned Google Colab is also available here for anyone who feels they want to take on the challenge of running this lecture on their machine.\n",
    "\n",
    "\n",
    "**Consider the following** before running this notebook on your computer:\n",
    "1. Make sure you have plenty of RAM (ideally >= 16 GB)\n",
    "\n",
    "2. If you **do not have** an NVIDIA GPU, you will have to install bitsandbytes cour version by following these steps\n",
    "    - In your terminal, activate your virtual environment and run `pip uninstall bitsandbytes` or in your notebook cell run `!pip uninstall bitsandbytes`\n",
    "    - In your terminal run `pip install bitsandbytes-cpu` or in your notebook cell run `!pip install bitsandbytes-cpu`\n",
    "\n",
    "3. If you **do** have an NVIDIA GPU\n",
    "    - In your terminal, activate your virtual environment and run `pip uninstall torch torchvision torchaudio` or in your notebook cell run `!pip uninstall torch torchvision torchaudio`\n",
    "    - In your terminal run `pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126` or in your notebook cell run `!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126`\n",
    "\n",
    "4. Make sure you have a stable internet connection as the download can take some time\n",
    "\n",
    "*Note* that running models without a GPU can be extremely time consuming and may lead to your machine overheating if it is old."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae931b",
   "metadata": {},
   "source": [
    "# Step 1: Load libraries and log in to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321f2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Previously we used pipelines to run models. Before pipelines were available, using the from_pretrained method was used. From_pretrained is a lower level method\n",
    "for achieving the same outcomes. pipelines is a much simpler method. Using this method you grab a model then grab a tokenizer to process the request. The \n",
    "tokenizer doesn't have to be from the same model but ideally it should be. \n",
    "\n",
    "Note, in the above comments the bitsandbytes-cpu install is not available so clearly something has changed. I could not get this to run. I had the same issue\n",
    "as before i.e. invalid layout parameter or something like that. I think it is a memory thing, as it chews up all my ram and dies. \n",
    "\n",
    "The following needs to be installed: -\n",
    "\n",
    "    pip install -q torch transformers bitsandbytes\n",
    "\n",
    "1.  torch runs the model weights and biases\n",
    "2.  transformers allows us to load and run the model and the tokenizer.\n",
    "3.  bitsandbytes allows us to quantize our model (basically decrease its size)\n",
    "\"\"\"\n",
    "\n",
    "# Load imports and get the hgginface token. \n",
    "import os\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2eef16",
   "metadata": {},
   "source": [
    "# Step 2: Load quantization configuration for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370475f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, BitsAndBytes only works with GPUs so it is removed if we are using CPU only.\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\"\"\" \n",
    "Quantization Config - this allows us to load the model into memory and use less memory\n",
    "\n",
    "Neural networks are trained on 32bit floating point precision. Quantization reduces the precision to 16, 8 or even 4bit which is what we are doing here. This\n",
    "also leads to lower precision or lower accuracy in the model but not by as much as the memory reduction. In this example (from 32bits to 4) the memory\n",
    "reduction is about 8 times. \n",
    "\"\"\"\n",
    "\n",
    "# this is the gpu version\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_quant_type=\"nf4\"\n",
    "# )\n",
    "\n",
    "# This is a CPU equivalent \n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float32,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7509a",
   "metadata": {},
   "source": [
    "# Step 3: Load a Pre-trained Model\n",
    "We will use the `meta-llama/Meta-Llama-3.1-8B-Instruct` model as an example. This step demonstrates how to load the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ee7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of importing pipeline, import this class which contains the from_pretrained method. \n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# pass in the model we want to use, device_map=\"auto\" to allow the function to detect whether we have a GPU or CPU only and use what is there, and the quantization from above. \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=quant_config)\n",
    "\n",
    "print(f\"Model '{model_name}' loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea53b6",
   "metadata": {},
   "source": [
    "# Step 4: Explore the Model Configuration\n",
    "The configuration of a model contains important details such as the number of layers, hidden size, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.config\n",
    "print(\"Model Configuration:\")\n",
    "print(config)\n",
    "\"\"\"\n",
    "The beginning of sentence token id, and three types of end of sentence token id. \n",
    "\n",
    "    \"bos_token_id\": 128000,\n",
    "    \"dtype\": \"float16\",\n",
    "    \"eos_token_id\": [\n",
    "        128001,\n",
    "        128008,\n",
    "        128009\n",
    "    ],\n",
    "\n",
    "  The activation function for the hidden layer\n",
    "\n",
    "    \"hidden_act\": \"silu\",\n",
    "\n",
    "The model type, and the number of hidden layers: -\n",
    "\n",
    "    \"model_type\": \"llama\",\n",
    "    \"num_attention_heads\": 32,\n",
    "    \"num_hidden_layers\": 32,\n",
    "\n",
    "The quantization method loaded above: -\n",
    "\n",
    "    \"quantization_config\": {\n",
    "        \"_load_in_4bit\": true,\n",
    "        \"_load_in_8bit\": false,\n",
    "        \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
    "        \"bnb_4bit_quant_storage\": \"uint8\",\n",
    "        \"bnb_4bit_quant_type\": \"nf4\",\n",
    "        \"bnb_4bit_use_double_quant\": true,\n",
    "        \"llm_int8_enable_fp32_cpu_offload\": false,\n",
    "        \"llm_int8_has_fp16_weight\": false,\n",
    "        \"llm_int8_skip_modules\": null,\n",
    "        \"llm_int8_threshold\": 6.0,\n",
    "        \"load_in_4bit\": true,\n",
    "        \"load_in_8bit\": false,\n",
    "        \"quant_method\": \"bitsandbytes\"\n",
    "    },\n",
    "\n",
    "    The vocabulary size, which I think is the number of words it recognizes: -\n",
    "\n",
    "        \"vocab_size\": 128256\n",
    "\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4acc9a",
   "metadata": {},
   "source": [
    "Let's have a look at the actual layers (just for fun!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows the configuration\n",
    "model\n",
    "\n",
    "\"\"\"\n",
    "LlamaForCausalLM(\n",
    "  (model): LlamaModel(\n",
    "    (embed_tokens): Embedding(128256, 4096)     <--- you can see the size of the vocabulary\n",
    "    (layers): ModuleList(\n",
    "      (0-31): 32 x LlamaDecoderLayer(           <--- and the number of layers\n",
    "        (self_attn): LlamaAttention(\n",
    "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
    "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
    "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
    "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
    "        )\n",
    "        (mlp): LlamaMLP(\n",
    "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
    "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
    "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
    "          (act_fn): SiLUActivation()\n",
    "        )\n",
    "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
    "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
    "      )\n",
    "    )\n",
    "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
    "    (rotary_emb): LlamaRotaryEmbedding()\n",
    "  )\n",
    "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
    ")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f042ff",
   "metadata": {},
   "source": [
    "# Step 5: Understand Caching\n",
    "When you load a model, it is cached locally to avoid downloading it again. The models are usually stored in the following path: ~/.cache/huggingface/hub by default i.e. C:\\Users\\damie\\.cache\\huggingface\\hub\n",
    "\n",
    "\n",
    "See further reference: [Huggingface cache management](~/.cache/huggingface/hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ec82c",
   "metadata": {},
   "source": [
    "# Step 6: Tokenizing a Prompt and Generating Text\n",
    "In this step, we will tokenize a list of messages, pass it to the model, and generate text as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e75830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An instruct model requires a list of messages as we saw in AI Engineering Essentials Part 1\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the best way to structure and organize my thoughts?\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30732686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1627,  10263,    220,   2366,     19,    271,   2675,    527,\n",
      "            264,  11190,  18328, 128009, 128006,    882, 128007,    271,   3923,\n",
      "            374,    279,   1888,   1648,    311,   6070,    323,  31335,    856,\n",
      "          11555,     30, 128009]])\n"
     ]
    }
   ],
   "source": [
    "# the messages need to be tokenized. This converts every word into its token number equivalent from its vocabulary.\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# works similar to downloading the model, in that you download the tokenizer for the model. \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# this is not needed. It just eliminates an anoying warning message in the output. \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# This does the work . Note I have commented out the to(\"cuda\") because i don't have a GPU\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\") ##.to(\"cuda\")\n",
    "\n",
    "print(inputs)\n",
    "\"\"\"\n",
    "And you get the results - a tensor with the words encoded. Notice the bos_token_id is the first number and one of the eos_token_id's is at the end. \n",
    "\n",
    "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
    "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
    "            220,   1627,  10263,    220,   2366,     19,    271,   2675,    527,\n",
    "            264,  11190,  18328, 128009, 128006,    882, 128007,    271,   3923,\n",
    "            374,    279,   1888,   1648,    311,   6070,    323,  31335,    856,\n",
    "          11555,     30, 128009]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5cd4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the input IDs to the model to generate output. The second parameter will limit the response to 80 words.\n",
    "output_ids = model(inputs, max_new_tokens=80)\n",
    "\n",
    "# Display the generated output IDs. This is still tokenized and not words. \n",
    "print(\"Generated Output IDs:\", output_ids)\n",
    "\n",
    "\"\"\"\n",
    "YOu get something like this: -\n",
    "\n",
    "    The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
    "    Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
    "    The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
    "    \n",
    "    Generated Output IDs: tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
    "            25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
    "            220,   1627,  10263,    220,   2366,     19,    271,   2675,    527,\n",
    "            264,  11190,  18328, 128009, 128006,    882, 128007,    271,   3923,\n",
    "            374,    279,   1888,   1648,    311,   6070,    323,  31335,    856,\n",
    "            11555,     30, 128009, 128006,  78191, 128007,    271,   9609,   1711,\n",
    "            323,  35821,    701,  11555,    649,    387,  17427,   1555,   5370,\n",
    "            12823,     13,   5810,    527,   1063,   7524,   5528,    311,   1520,\n",
    "            499,  38263,    323,  63652,    701,   6848,   1473,     16,     13,\n",
    "            3146,  70738,  39546,  96618,   5256,    449,    264,   8792,   4623,\n",
    "            323,   1893,    264,   9302,   2472,    315,   5552,  19476,     11,\n",
    "            1701,  23962,     11,  21513,     11,    323,   5448,     13,   1115,\n",
    "            15105,   8779,    311,  10765,  12135,    323,  13537,   1990,   6848,\n",
    "            627,     17,     13,   3146,  66177,  27511,    287,  96618,   9842,\n",
    "            1523,    439]], device='cuda:0')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4dae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No turn the output id's back to words, but don't return the special tokens. The [0] just returns the inner list.\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\", generated_text)\n",
    "\n",
    "\"\"\"\n",
    "If you include special tokens, you get this\n",
    "\n",
    "Generated Text: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 26 Jul 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What is the best way to structure and organize my thoughts?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "Structuring and organizing your thoughts can be achieved through various techniques. Here are some effective methods to help you clarify and prioritize your ideas:\n",
    "\n",
    "1. **Mind Mapping**: Start with a central idea and create a visual map of related concepts, using branches, keywords, and images. This technique helps to identify relationships and connections between ideas.\n",
    "2. **Brainstorming**: Write down as\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
