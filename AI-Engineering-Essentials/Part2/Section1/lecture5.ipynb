{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d94fb4",
   "metadata": {},
   "source": [
    "# Lecture 5: Coding with Huggingface\n",
    "\n",
    "This lecture will take place on Google Colab for enhanced processing power\n",
    "\n",
    "https://colab.research.google.com/drive/1XNC5i1056nq_r8pvJjf2EAsmKw9bGOMD\n",
    "\n",
    "Note, you need to install the following: -\n",
    "\n",
    "* pip install -q transformers diffusers torch\n",
    "\n",
    "Transformers and diffusers are huggingface libraries.\n",
    "\n",
    "1. Transformers is a powerful Python library created by Hugging Face that allows you to download, manipulate, and run thousands of pretrained, open-source AI models. These models cover multiple tasks across modalities like natural language processing, computer vision, audio, and multimodal learning. You defined a task (like text-generation), of which there are about 15 of them, and if you don't specify a model it will use a default model. Below we use a number of them. See [Documentation](https://huggingface.co/docs/transformers/v4.57.1/en/main_classes/pipelines#transformers.Pipeline) for a list of all tasks.\n",
    "2. Diffusion-based models are a class of generative models that work by progressively adding noise to data and then reversing the process to generate new, high-quality data. They have become popular in AI for generating images, videos, and other data types. See [Documentation](https://huggingface.co/docs/diffusers/using-diffusers/unconditional_image_generation) for the types of image generation you can do.\n",
    "\n",
    "These download the weights and biases, but they need either pytorch or tensorflow to run the matrix opperations for the weights and biases. The -q option means do it quietly i.e. don't show any output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03b57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv \n",
    "from huggingface_hub import login\n",
    "\n",
    "# load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# and get the API key\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "if HUGGINGFACE_API_KEY is None:\n",
    "    raise Exception(\"API key is missing\")\n",
    "\n",
    "# And log in to huggingface\n",
    "login(token=HUGGINGFACE_API_KEY)\n",
    "\n",
    "# The pipeline function is the quickest way to get started with any of the models from huggingface\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312417c7",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af315f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Seer\\J Notebooks\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: \n",
      " The secret to baking a good cake is vernacular.\n",
      "\n",
      "Just make sure you don't mess with the dough.\n",
      "\n",
      "If you are going to use a plastic bag, make sure it is sturdy and doesn't get twisted.\n",
      "\n",
      "If you are going to use baking soda, make sure it is well-ventilated.\n",
      "\n",
      "If you are going to use a mix of corn starch, butter and baking soda, make sure you mix well.\n",
      "\n",
      "Make sure you leave enough space for the butter to stick to the edges of the dough.\n",
      "\n",
      "This cake is a bit trickier to do, so don't be afraid to use a larger bag.\n",
      "\n",
      "When it comes to baking cakes, I've used a butter-filled bag that's about 5.5 inches thick. It comes in a variety of sizes, but the one that comes with the Cake Bowl is 4.5 inches.\n",
      "\n",
      "If you're making these cakes in a large bowl, you can use a smaller bag.\n",
      "\n",
      "If you're making these cakes in a small bag, you can use a larger bag.\n",
      "\n",
      "If you're making these cakes in a larger bowl, you can use a smaller bag.\n",
      "\n",
      "If you are making these cakes in a larger bowl, you can use a smaller bag.\n"
     ]
    }
   ],
   "source": [
    "# the quickest way is to identify the task you want to perform and let the library pick the model for you. Text generation is not like a asking a question and getting\n",
    "# an answer. It is more like autocomplete. You give it a prompt and it generates text based on that prompt.\n",
    "generator = pipeline(\"text-generation\")\n",
    "# Generate a maximum of 200 tokens\n",
    "gen_output = generator(\"The secret to baking a good cake is \", max_length=200)[0]['generated_text']\n",
    "print('\\nOutput: \\n',gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4cfd5",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a CPU, remove device=\"cuda\"\n",
    "# summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
    "summarizer = pipeline(\"summarization\")\n",
    "sum_output = summarizer(gen_output, max_length=50)\n",
    "print(sum_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047267ed",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc741ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a CPU, remove device=\"cuda\". You actually don't need to specify \"text-classification\" because the model is already fine-tuned for that task.\n",
    "#classifier = pipeline(model=\"ProsusAI/finbert\", device=\"cuda\")\n",
    "classifier = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "class_output = classifier(\"I really want this to be done\", max_length=100)\n",
    "print(class_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba27c5",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e0a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a CPU, remove device=\"cuda\"\n",
    "#translator = pipeline(\"translation_en_to_fr\", model=\"google-t5/t5-base\", device=\"cuda\")\n",
    "\n",
    "# This one failed to work. It didn't start downloading model.safetensors. \n",
    "translator = pipeline(\"translation_en_to_fr\", model=\"google-t5/t5-base\")\n",
    "translator_output = translator(\"Fly me to the moon\", max_length=100)\n",
    "print(translator_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f61da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cant get this to work. But it worked in colab but took 16 minutes.\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "# an image generator\n",
    "image_gen = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "\n",
    "# generate the image\n",
    "image_gen = image_gen(\"A fire cat riding on a spaceship\").images[0]\n",
    "# and display\n",
    "image_gen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
