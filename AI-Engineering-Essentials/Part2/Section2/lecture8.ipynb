{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4734b0b5",
   "metadata": {},
   "source": [
    "# Lecture 8: Assembling our AI\n",
    "\n",
    "Link to Google Colab notebook: https://colab.research.google.com/drive/1NMJua1cYrqUmh_JjQqupp3UCUCFmVMAL?usp=sharing\n",
    "\n",
    "The purpose of this project is to create a Note Taker user interface where we can speak into the application and use one model to convert speech to text. Then pass the text into another model to summarize it i.e. as notes. Wrap the whole thing in a Gradio interface.\n",
    "\n",
    "The lab implemented using a meta model downloaded from huggingface. However, that failed on my laptop and kept timing out in google colab so I h\n",
    "re-written it to use and Open AI model to summarize my audio notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721efb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load imports and get the Open AI token. \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb3cedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI   # So we can use the open api whisper model to capture my speach.\n",
    "from pathlib import Path    # Then us this to save the audit to a file and return the path\n",
    "\n",
    "# connect to open AI\n",
    "openai_client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "# Create the function to convert my recorded audio to text. \n",
    "def transcribe_audio(audio):\n",
    "    audio_file = Path(audio)  # Save the audio to a file\n",
    "    # Create the transcriber and convert the audio to text\n",
    "    transcriber = openai_client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file\n",
    "    )\n",
    "    # and return to the caller\n",
    "    transcription = transcriber.text\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7bf0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is in the format expected by the Gradio submit_button.click per below \n",
    "#   submit_button.click(fn = notes_from_audio, inputs=[audio_input], outputs=[transcription]) code below. \n",
    "# It is passed the audio I previously recorded. The function transcribes it then converts it to notes. \n",
    "def notes_from_audio(audio=None):\n",
    "    transcription = transcribe_audio(audio)\n",
    "\n",
    "    SYSTEM_PROMPT_MESSAGE = \"You are an expert notetaker, use the following transcription from my audio to generate \"\n",
    "    SYSTEM_PROMPT_MESSAGE += \"notes based on it. Make sure to keep these brief using bullet points.\"\n",
    "    prompt = SYSTEM_PROMPT_MESSAGE + \"/nTranscription/n\" + transcription\n",
    "    message = [{'role':'user','content':prompt}]\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",    # Use the mini version as it is more cost effective\n",
    "        messages=message,       # the audio\n",
    "        max_tokens=100,         # the max length of the response\n",
    "        temperature=0.7         # the higher the temp the more creative the response. the lower the temp the more precise the response \n",
    "                                # which means your are more likely to get the same response for the same prompt\n",
    "    )\n",
    "    # It looks like this. \n",
    "    # ChatCompletion(id='chatcmpl-CR6SEm8gzlKrpBUx7pdLGP2FKMzAR', \n",
    "    #       choices=[Choice(finish_reason='stop', index=0, logprobs=None, \n",
    "    #           message=ChatCompletionMessage(content='- Testing audio recording for text conversion\\n- Objective: Summarize recording into . . .)))\n",
    "    # So unpick as follows: -\n",
    "    notes = response.choices[0].message.content\n",
    "    return notes, transcription\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab47c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d205665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Break the application into blocks.\n",
    "with gr.Blocks() as demo:\n",
    "  # will create 3 rows in the block. The first captures the audio. \n",
    "  with gr.Row():\n",
    "    audio_input = gr.Audio(sources=[\"upload\",\"microphone\"], type=\"filepath\", label=\"Audio Transcriber\")\n",
    "  \n",
    "  # The second is two text boxes to hold the notes and transcriptions. The text box objects are saved to the variables\n",
    "  # transcription and notes so they can be referenced by the submit button. \n",
    "  with gr.Row(height=500):\n",
    "    transcription = gr.Textbox(label=\"Transcription\", lines=10)\n",
    "    notes = gr.Textbox(label=\"Notes\", lines=10)\n",
    "\n",
    "  # The thrid is a submit button that takes the audio above, converts it to text and summarizes that text into notes.\n",
    "  # It then saves the text into the notes and transcription fields above. The outputs references the text box by its label\n",
    "  with gr.Row():\n",
    "    submit_button = gr.Button(\"Start\")\n",
    "    submit_button.click(fn = notes_from_audio, inputs=[audio_input], outputs=[notes, transcription])\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
