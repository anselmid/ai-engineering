{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afaf9690",
   "metadata": {},
   "source": [
    "# Lecture 7: AI Notetaker - Building Blocks\n",
    "\n",
    "Link to google colab: https://colab.research.google.com/drive/1PKVKD6pSm5e-HW1dnqmkygtfXVHv13zH?usp=sharing\n",
    "\n",
    "We are using these. they are installed already: -\n",
    "\n",
    "* pip install -q torch transformers bitsandbytes huggingface_hub gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabd3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load imports and get the hgginface token. \n",
    "import os\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf46c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "# use the pipeline for efficiency. Still getting KeyError: 'layout'. Assume it is a memory issue. \n",
    "# The instruct version of this model is fine tuned for following instructions like a typical chat experience. \n",
    "chatbot = pipeline(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,     # \n",
    "    device_map=\"auto\",              # Chooses GPU if there, or CP\n",
    "    model_kwargs={\"quantization_config\": BitsAndBytesConfig(load_in_4bit=True)},    # shrink the RAM requirement for the model by making it 4bit\n",
    "    # Watch out for pad_token_id=128001. This is particularly for the model meta-llama/Llama-3.1-8B-Instruct.\n",
    "    # If you were to choose another model, you will have to get the correct eos_token_id for that model.\n",
    "    # This is the token id of the end of sentence token. This gets rid of an annoying error that pops up. \n",
    "    pad_token_id=128001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c1c05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'What is a current existential crisis faced by humanity?'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the message\n",
    "prompt = \"What is a current existential crisis faced by humanity?\"\n",
    "\n",
    "messages = [\n",
    "    {'role':'user','content':prompt}\n",
    "]\n",
    "# and view\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the response and print. Limit to 50 tokens to simplify\n",
    "response = chatbot(messages, max_new_tokens=50)\n",
    "print(response[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e90b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a chat bot. \n",
    "messages = []\n",
    "user_input= \"\"\n",
    "\n",
    "while user_input != 'bye':\n",
    "  user_input = str(input(\"You: \"))  # get user input\n",
    "  messages.append({'role':'user', 'content':user_input})  # append it to the list with the role of user\n",
    "  ai_response = chatbot(messages, max_new_tokens=50)  # generate the response.\n",
    "  # and up pick the final response. [0] gets us the entire response from the outer list. \n",
    "  # ['generated_text'] gets us the generated_text from the inner list\n",
    "  # [-1] gets us the final AI response from all the data with ['content'] returning the specific content of that response. \n",
    "  ai_response = ai_response[0]['generated_text'][-1]['content']\n",
    "  print(\"AI: \", ai_response) # Dipslay it\n",
    "  messages.append({'role':'assistant','content':ai_response}) # and append it to the messages so we can pass in the full context next time around\n",
    "\n",
    "\"\"\"\n",
    "An example of how this might look: -\n",
    "\n",
    "    You: hi there\n",
    "    AI:  How are you today? Is there something I can help you with or would you like to chat?\n",
    "    You: what are you capable of?\n",
    "    AI:  I'm a large language model, I can perform a wide range of tasks, including:\n",
    "\n",
    "    1. **Answering questions**: I can provide information on various topics, from science and history to entertainment and culture.\n",
    "    2. **Generating text**: I can\n",
    "    You: continue your sentence\n",
    "    AI:  ...generate text on a given topic, summarize long pieces of text, create content, and even assist with writing tasks like proofreading and editing.\n",
    "\n",
    "    3. **Translation**: I can translate text from one language to another, helping to bridge language gaps.\n",
    "\n",
    "    You: bye\n",
    "    AI:  Bye! It was nice chatting with you. If you need anything else, feel free to come back anytime. Have a great day!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d0fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the above but this time print what is being passed in\n",
    "from pprint import pprint\n",
    "messages = []\n",
    "user_input = ''\n",
    "\n",
    "while user_input != 'bye':\n",
    "  user_input = input('You: ')\n",
    "  messages.append({'role':'user','content':user_input})\n",
    "  print('\\nWhat is being passed to the model?\\n')\n",
    "  pprint(messages)\n",
    "  print('\\n')\n",
    "  assistant = chatbot(messages)[0]['generated_text'][-1]['content']\n",
    "  print(\"AI: \", assistant)\n",
    "  messages.append({'role':'assistant','content':assistant})\n",
    "\n",
    "\"\"\"\n",
    "It looks like this: -\n",
    "\n",
    "    You: hi there\n",
    "\n",
    "    What is being passed to the model?\n",
    "\n",
    "    [{'content': 'hi there', 'role': 'user'}]\n",
    "\n",
    "\n",
    "    AI:  How are you today? Is there something I can help you with or would you like to chat?\n",
    "    You: what time in the morning is best for working out?\n",
    "\n",
    "    What is being passed to the model?\n",
    "\n",
    "    [{'content': 'hi there', 'role': 'user'},\n",
    "    {'content': 'How are you today? Is there something I can help you with or '\n",
    "                'would you like to chat?',\n",
    "    'role': 'assistant'},\n",
    "    {'content': 'what time in the morning is best for working out?',\n",
    "    'role': 'user'}]\n",
    "\n",
    "\n",
    "    AI:  Research suggests that the best time for a workout is early in the morning, typically between 5-\n",
    "    You: please continue your thought\n",
    "\n",
    "    What is being passed to the model?\n",
    "\n",
    "    [{'content': 'hi there', 'role': 'user'},\n",
    "    {'content': 'How are you today? Is there something I can help you with or '\n",
    "                'would you like to chat?',\n",
    "    'role': 'assistant'},\n",
    "    {'content': 'what time in the morning is best for working out?',\n",
    "    'role': 'user'},\n",
    "    {'content': 'Research suggests that the best time for a workout is early in '\n",
    "                'the morning, typically between 5-',\n",
    "    'role': 'assistant'},\n",
    "    {'content': 'please continue your thought', 'role': 'user'}]\n",
    "\n",
    "\n",
    "    AI:  ...7 am. This is often referred to as the \"golden hour\" for working out. Here\n",
    "    You: bye\n",
    "\n",
    "    What is being passed to the model?\n",
    "\n",
    "    [{'content': 'hi there', 'role': 'user'},\n",
    "    {'content': 'How are you today? Is there something I can help you with or '\n",
    "                'would you like to chat?',\n",
    "    'role': 'assistant'},\n",
    "    {'content': 'what time in the morning is best for working out?',\n",
    "    'role': 'user'},\n",
    "    {'content': 'Research suggests that the best time for a workout is early in '\n",
    "                'the morning, typically between 5-',\n",
    "    'role': 'assistant'},\n",
    "    {'content': 'please continue your thought', 'role': 'user'},\n",
    "    {'content': '...7 am. This is often referred to as the \"golden hour\" for '\n",
    "                'working out. Here',\n",
    "    'role': 'assistant'},\n",
    "    {'content': 'bye', 'role': 'user'}]\n",
    "\n",
    "\n",
    "    AI:  Have a great day!\n",
    "\n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The way gradio works is that you need to craft a function that meets the requirements for its out of the box interface\n",
    "# messages is the new message entered by the user. We build up the history by adding the messages to the history. \n",
    "# we use the model to generate a response and return it (the full response i.e. \n",
    "# {'content': 'How are you today? Is there something I can help you with or would you like to chat?', 'role': 'assistant'})\n",
    "# I am guessing the gradio adds this to the history.\n",
    "def chat(messages, history):\n",
    "  history.append({'role':'user','content':messages})\n",
    "  ai_response = chatbot(history, max_new_tokens=50)[0]['generated_text'][-1]\n",
    "  return ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bacea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use gradio to display a better interface.\n",
    "import gradio as gr\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn = chat,\n",
    "    type=\"messages\"\n",
    ")\n",
    "\n",
    "demo.launch(debug=True) # If you get an error, adding debug=True helps you resolve it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
